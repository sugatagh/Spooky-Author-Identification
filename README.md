# Spooky Author Identification

## Kaggle Notebook

[**Spooky Author Identification (GloVe + LSTM)**](https://www.kaggle.com/code/sugataghosh/spooky-author-identification-glove-lstm)

## Overview
 
Suppose that we are given a specific text and we only know that the author of the text is one among [**Edgar Allan Poe**](https://en.wikipedia.org/wiki/Edgar_Allan_Poe) $(\text{EAP})$, [**H. P. Lovecraft**](https://en.wikipedia.org/wiki/H._P._Lovecraft) $(\text{HPL})$ and [**Mary Shelley**](https://en.wikipedia.org/wiki/Mary_Shelley) $(\text{MWS})$. How do we predict who wrote the text? More specifically, how to predict the probability that the given text is written by Edgar Allan Poe, and the same for the other two authors?

In this work, we have a large dataset of texts labeled with the true author, who is one among $\text{EAP}$, $\text{HPL}$ and $\text{MWS}$. The data is provided in the **Dataset** folder of the repository. However, in the notebook, it is imported directly from the [**Spooky Author Identification**](https://www.kaggle.com/c/spooky-author-identification) Kaggle competition. The objective is to train a model to predict probabilities that a given new text is written by $X$, where $X$ = $\text{EAP}$, $\text{HPL}$ and $\text{MWS}$. We assume that the new text is indeed written by one of the authors, so that the three probabilities add up to $1$. This immediately helps us in classifying the given text as written by a specific author, for instance, we can choose the author with the highest probability of writing the text as a prediction.

We use this problem to illustrate the use of two relevant techniques: [**GloVe**](https://en.wikipedia.org/wiki/GloVe) model for [**word vectorizations**](https://en.wikipedia.org/wiki/Word_embedding) and [**long short-term memory**](https://en.wikipedia.org/wiki/Long_short-term_memory) (LSTM) [**neural network**](https://en.wikipedia.org/wiki/Artificial_neural_network) for **model building**. The steps in this notebook towards the mentioned objective are as follows:

- We define the `multiclass_log_loss` function, which takes in a matrix of binarized true target classes `y_true_binarized`, a matrix of predicted class probabilities `y_pred_probabilities` and a clipping parameter `epsilon`, and produce the multiclass version of the [**log loss**](https://scikit-learn.org/stable/modules/model_evaluation.html#log-loss) metric between `y_true_binarized` and `y_pred_probabilities`. To utilize this function as [**loss**](https://keras.io/api/losses/) in **model compilation**, we use [**TensorFlow**](https://en.wikipedia.org/wiki/TensorFlow) and **Keras backend** functions to write it, instead of the standard [**NumPy**](https://en.wikipedia.org/wiki/NumPy) functions.

- We [**split the data**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) in $80:20$ ratio (the [**training set**](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets) consisting of $80\%$ data, and the [**validation set**](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets) consisting of the rest). We [**stratify**](https://en.wikipedia.org/wiki/Stratified_sampling) the split using the labels, so that the proportion of each label remains roughly the same in the training set and the validation set.

- We encode the labels $\text{EAP}$, $\text{HPL}$ and $\text{MWS}$ using a dictionary and map them to integer values $0$, $1$ and $2$, respectively; and convert the integer **label vectors** to **binary class matrices**, each row of which represents a [**one-hot**](https://en.wikipedia.org/wiki/One-hot) vector, corresponding to an integer component of the label vector.

- We fit [**Keras tokenizer**](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) on the combined list of texts from the training set and the validation set. The obtained words are then indexed by employing the **word_index** method; convert the texts to sequences of integers using the **texts_to_sequences** method; and use the [**pad_sequences**](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) function of [**Keras**](https://en.wikipedia.org/wiki/Keras) to pad the sequences to a maximum length to be equal to the smallest integer greater than $m + 2s$, where $m$ and $s$ respectively denote the [**mean**](https://en.wikipedia.org/wiki/Mean) and [**standard deviation**](https://en.wikipedia.org/wiki/Standard_deviation) of the text lengths from the combined set of texts from the training set and the validation set. We construct a matrix of vector representations of the words found in the training set and the validation set by mapping the words to a $100$-dimensional vector space through **GloVe embedding**.

- We build a [**sequential model**](https://www.tensorflow.org/guide/keras/sequential_model) consisting of an [**embedding layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) with weights provided by the matrix of word vectors, constructed previously; a [**SpatialDropout1D layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout1D); an [**LSTM layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) with number of units same as the length of the GloVe vectors; two [**dense**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) **hidden layers** with [**ReLU**](https://www.tensorflow.org/api_docs/python/tf/nn/relu) [**activation function**](https://en.wikipedia.org/wiki/Activation_function), each followed by a **dropout layer**; and an **output layer** of three [**neurons**](https://en.wikipedia.org/wiki/Artificial_neuron), corresponding to the three probabilities for the three authors, with [**softmax**](https://en.wikipedia.org/wiki/Softmax_function) activation function. The model is compiled with the manually defined `multiclass_log_loss` function as **loss** and the [**Adam**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) [**optimizer**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/) with an initial learning rate of $0.001$, which is then regulated by a manually defined [**schedule**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules) function `scheduler_modified_exponential` through [**learning rate scheduler**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) **callback** to update the [**learning rate**](https://en.wikipedia.org/wiki/Learning_rate) for the optimizer at each epoch.

- We fit the model on the padded sequences generated from the training texts and the binary class matrix generated from the training labels for a set number of epochs. The training loss and the validation loss is monitored at each epoch and we stop the training procedure once the validation loss stops improving via an [**early stopping**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) [**callback**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/). We produce a plot depicting how the training loss and the validation loss evolved over epochs, giving an overall picture of the model building procedure.

- We employ the trained model to predict the probabilities of the texts, in both the training set and the validation set, being written by the three authors and obtain a training log loss of $0.391$ and validation log loss of $0.581$. The predicted probabilities are then converted to labels by picking the **mode** and we get a training [**accuracy**](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy) of $0.846$ and validation accuracy of $0.764$. Finally, a complete picture of the performance of the trained model on the validation set, in the context of the task of classifying the texts as written by one of the three authors, is provided through a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix).

## Acknowledgements

- [**Dataset**](https://www.kaggle.com/competitions/spooky-author-identification/data) provided in the Kaggle competition [**Spooky Author Identification**](https://www.kaggle.com/c/spooky-author-identification)
- [**GloVe: Global Vectors for Word Representation**](https://www.kaggle.com/datasets/rtatman/glove-global-vectors-for-word-representation) dataset by [**Rachael Tatman**](https://www.kaggle.com/rtatman)
- [**How to Choose a Learning Rate Scheduler for Neural Networks**](https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler) by **Yi Li**
- [**Practical Recommendations for Gradient-Based Training of Deep
Architectures**](https://arxiv.org/pdf/1206.5533.pdf) by [**Yoshua Bengio**](https://en.wikipedia.org/wiki/Yoshua_Bengio)

## Further Reading

- [**GloVe: Global Vectors for Word Representation**](https://nlp.stanford.edu/pubs/glove.pdf), by [**Jeffrey Pennington**](https://nlp.stanford.edu/~jpennin/), [**Richard Socher**](https://www.socher.org/) and [**Christopher D. Manning**](https://nlp.stanford.edu/~manning/)
- [**Glove Research Paper Clearly Explained**](https://towardsdatascience.com/glove-research-paper-clearly-explained-7d2c3641b8a6), by [**Meesala Lokesh**](https://medium.com/@meesala.lokesh)
- [**Long Short-Term Memory**](https://www.bioinf.jku.at/publications/older/2604.pdf), by [**Sepp Hochreiter**](https://en.wikipedia.org/wiki/Sepp_Hochreiter) and [**Jurgen Schmidhuber**](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber)
- [**The Unreasonable Effectiveness of Recurrent Neural Networks**](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), by [**Andrej Karpathy**](https://karpathy.ai/)
- [**Understanding LSTM Networks**](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), by [**Christopher Olah**](https://colah.github.io/about.html)
 
